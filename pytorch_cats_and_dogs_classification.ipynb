{
  "cells": [
    {
      "metadata": {
        "id": "Uf95LFdyxc86"
      },
      "cell_type": "markdown",
      "source": [
        "![image-classification-using-transfer-learning-in-pytorch.png](attachment:image-classification-using-transfer-learning-in-pytorch.png)\n",
        "\n",
        "## PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab.\n",
        "\n",
        "![pytoch-cnn.jpg](attachment:pytoch-cnn.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrhOWbvfxyv1",
        "outputId": "fbbfec07-a676-4a09-aab1-912190816943"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7FqYJO9yjHV",
        "outputId": "976cb726-09b1-48e9-faab-0f37b6b20072"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/data.zip\n",
            "   creating: data/\n",
            "   creating: data/train/\n",
            "   creating: data/train/cat/\n",
            "  inflating: data/train/cat/cat-red-cute-mackerel.jpg  \n",
            "  inflating: data/train/cat/fox-818.jpeg  \n",
            "  inflating: data/train/cat/HD-wallpaper-tiger-predator-muzzle-big-cat-glance.jpg  \n",
            "  inflating: data/train/cat/Header_1920x380_FutterergaCC88nzungKatze_Neu.png  \n",
            "  inflating: data/train/cat/kittens-cat-cat-puppy-rush-45170.jpeg  \n",
            "  inflating: data/train/cat/kitty-cat-kitten-pet-45201.jpeg  \n",
            "  inflating: data/train/cat/lynx-white_191971-9235.jpg  \n",
            "  inflating: data/train/cat/pexels-photo-1031460.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1047369.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1084425.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1170986.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1255093.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-134060.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1358845.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1366996.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1461622.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-14721098.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1472999.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1499344.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1510543.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1521306.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-15379284.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1543793.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1543801.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-15445009.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1560424.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1571076.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1643457.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-165752.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1687831.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1693441.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-16934412.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-171227.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1712272.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1715092.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1735057.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-17350572.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-173909.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-1741205.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2019947.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2061057.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-20610572.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2064110.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-20641102.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2065357.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-20653572.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2071873.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2071881.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2071882.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2077372.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2083940.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-208773.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2087732.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-208805.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-208984.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-209037.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2113332.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2123433.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2127433.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-21274332.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-21738723.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2181171.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-21811712.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2194261.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-21942612.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2254062.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2278093.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-22780932.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2286016.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-22860162.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2361952.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-236587.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-236603.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2366032.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-236606.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2366062.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2366602.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-247502.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2482802.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-248304.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2483042.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2483502.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-2499282.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-24992823.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-256632.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3318215.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3335621.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-33356212.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-35252983.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3571412.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3616232.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-36162322.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3643714.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-36437143.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-368890.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3688902.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3712095.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-37120952.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-3777622.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-384555.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-416160.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-416195.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-596590.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-709481.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7149465.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-71494652.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-720684.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7206842.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-730896.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7308962.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7315532.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7354232.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7365302.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7365322.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7410722.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-74107222.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7477952.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-751050.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-7510502.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-8985189.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-9709427.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-982314.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo-991831.jpeg  \n",
            "  inflating: data/train/cat/pexels-photo3.jpg  \n",
            "  inflating: data/train/cat/pngtree-cute-cat-side-face-material-picture-image_1728650.jpg  \n",
            "  inflating: data/train/cat/stray_cat.jpg  \n",
            "  inflating: data/train/cat/tabby-cat-close-up-portrait-69932.jpeg  \n",
            "  inflating: data/train/cat/the-lucky-neko-WNoYQaAtCfo-unsplash.jpg  \n",
            "  inflating: data/train/cat/tumblr_6a05b5446d1004998a7b362c210b7281_898d63ee_2048.jpg  \n",
            "   creating: data/train/dog/\n",
            "  inflating: data/train/dog/furry-husky-dog-85b0250uluw1y5w8.jpg  \n",
            "  inflating: data/train/dog/garden-dog-pet.jpg  \n",
            "  inflating: data/train/dog/isla_fullxfull.47303867_emvlek2p.jpg  \n",
            "  inflating: data/train/dog/pexels-photo-1009405.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1031431.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-10314312.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1078089.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-10780892.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1125766.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1126479.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-11264792.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1147812.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1191000.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-119592.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1287831.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-128817.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1289555.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1289556.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1289557.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1289905.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1294062.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1379052.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1383811.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-14202552.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1458913.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1458914.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-14920486.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-15057085.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-15096714.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1510210.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-15147702.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1626219.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1629781.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1641862.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-164446.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1644462.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1684504.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1694156.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-169524.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1714454.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-17144542.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-17412352.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1750378.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1750543.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-17505432.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1751541.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-1805165.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2149749.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2326938.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-235805.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2366222.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-243914.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-245035.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-247937.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-247968.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2575403.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2575772.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2791658.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2802417.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2803294.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-2873386.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-290720.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-295028.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3104708.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3299904.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3376610.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3427087.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3478875.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3487734.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3489061.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3498989.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-34989892.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3649170.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3714060.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-37140602.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3735238.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3812207.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-38122072.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3813324.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3851211.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3860306.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3866500.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-38665002.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3908765.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-3908808.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-4056462.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-4587967.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-4587977.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-4587979.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5256722.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5257587.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5288556.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-53261.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-53769.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-540521.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5405212.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-54199.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-542142.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-544269.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-545063.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5462282.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5769317.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-58870.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-589972.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-5945832.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-594687.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-604532.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-605496.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-6635732.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-666870.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-6668702.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-668004.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-686094.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-68798.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-6886942.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-69372.jpeg  \n",
            "  inflating: data/train/dog/pexels-photo-697736.jpeg  \n",
            "  inflating: data/train/dog/piper-4763.jpg  \n",
            "  inflating: data/train/dog/portrait-of-a-cute-dog-in-studio-photo.jpg  \n",
            "  inflating: data/train/dog/s-animal-wildlife-wild-mammal-darkness-wolf-fox-fauna-dog-like-mammal-983125.jpg  \n",
            "  inflating: data/train/dog/schnouzer_1400x.jpg  \n",
            "  inflating: data/train/dog/shutterstock_334451795.jpg  \n",
            "  inflating: data/train/dog/west-highland-terrier_309409-3841.jpg  \n",
            "  inflating: data/train/dog/wolf-predator-wildlife-montana-686692.jpeg  \n",
            "   creating: data/val/\n",
            "   creating: data/val/cat/\n",
            "  inflating: data/val/cat/08a2f0101c5e7a3d11b03c7b5e78c353.jpg  \n",
            "  inflating: data/val/cat/2f04e363042a1bf7e453f3d2ee3c126e.jpg  \n",
            "  inflating: data/val/cat/5140017192_8d730bda18_b.jpg  \n",
            "  inflating: data/val/cat/812110dc00bbe8277dff278f522cd19f.jpg  \n",
            "  inflating: data/val/cat/aqf487tw6zt71.jpg  \n",
            "  inflating: data/val/cat/cat-5101520_1280.jpg  \n",
            "  inflating: data/val/cat/cat-animal-cat-portrait-mackerel.jpg  \n",
            "  inflating: data/val/cat/cat-animals-cats-portrait-of-cat2.jpg  \n",
            "  inflating: data/val/cat/cat-fold-view-grey-fur.jpg  \n",
            "  inflating: data/val/cat/cat-silhouette-cats-silhouette-cat-s-eyes.jpg  \n",
            "  inflating: data/val/cat/cat-sweet-kitty-animals-57416.jpeg  \n",
            "  inflating: data/val/cat/cat-throwing-up-food-but-acting-normal.jpg  \n",
            "  inflating: data/val/cat/cat_emotions_cute_136463_2780x2780.jpg  \n",
            "  inflating: data/val/cat/croatia-zagreb-catter-shelf.jpg  \n",
            "  inflating: data/val/cat/cute-cartoon-black-cat-isolated_71599-2198.jpg  \n",
            "  inflating: data/val/cat/cute-cat-3d-rendering-free-png.png  \n",
            "  inflating: data/val/cat/cute-ginger-kitten-peeks-out-from-edge-white-board-copy-space_96064-786.jpg  \n",
            "  inflating: data/val/cat/cy93ZWJzaXRlX2NvbnRlbnQvcHUyMzMxNjM2LWltYWdlLTAxLXJtNTAzXzMtbDBqOXFrNnEucG5n.png  \n",
            "  inflating: data/val/cat/donartro-sottosopra-mensole-percorso-gatti2.jpg  \n",
            "   creating: data/val/dog/\n",
            "  inflating: data/val/dog/-cartoon-malinois-handdrawn-pet-animals-comic-vector-illustration_708561-753.jpg  \n",
            "  inflating: data/val/dog/2448490111_215e91bdf3_z.png  \n",
            "  inflating: data/val/dog/635605708412105941-Crystal.jpg  \n",
            "  inflating: data/val/dog/benefits_of_neutering_or_spaying_a_dog_1636352788.png  \n",
            "  inflating: data/val/dog/BpeGVsX2ltYWdlcy93ZWJzaXRlX2NvbnRlbnQvcHUyMzMxNzkwLWltYWdlLWpvYjYxNV8xLnBuZw.png  \n",
            "  inflating: data/val/dog/close-up-akita-inu-puppy-front-white-wall_191971-19106.jpg  \n",
            "  inflating: data/val/dog/comparingthe.jpg  \n",
            "  inflating: data/val/dog/cute-doodle-illustration-of-basset-hound-breed-dog-dog-in-minimalist-style-vector.jpg  \n",
            "  inflating: data/val/dog/cy93ZWJzaXRlX2NvbnRlbnQvcHUyMzMxNzg4LWltYWdlLXJtNTAzLTAxXzEtbDBqOXFyYzMucG5n.png  \n",
            "  inflating: data/val/dog/dalmatian-dog-photo.jpg  \n",
            "  inflating: data/val/dog/db-dog-2-e6ffb59fd837f0e7d032bcd093347ac0f59e28ce41c43031269e983cb2e6326e.jpg  \n",
            "  inflating: data/val/dog/dog-bucket-list.jpg  \n",
            "  inflating: data/val/dog/dog-cute-pet.jpg  \n",
            "  inflating: data/val/dog/dog-golden-retriver-dog-portrait-beauty-38538.jpeg  \n",
            "  inflating: data/val/dog/dog-outline-illustration-coloring-book-page-coloring-card-kids-adults-generative-ai_742252-6398.jpg  \n",
            "  inflating: data/val/dog/dog-puppy-animal-portrait-small-dog-59988.jpeg  \n",
            "  inflating: data/val/dog/dogfamilyphotographerlieslquinnweb286of1029.jpg  \n",
            "  inflating: data/val/dog/dog_PNG50337.png  \n",
            "  inflating: data/val/dog/download_4c795670-96f7-4d77-9232-7e0b89efa0c1.jpg  \n",
            "  inflating: data/val/dog/Earth-Animal-Flea-on-Derm-e1531931955968.jpg  \n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ieTB9GQpxc8_"
      },
      "cell_type": "markdown",
      "source": [
        "## Import all dependencies"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "i4bUPAnyxc8_"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torchvision.datasets as datasets # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms # Transformations we can perform on our dataset\n",
        "import torch.nn.functional as F # All functions that don't have any parameters\n",
        "from torch.utils.data import DataLoader, Dataset # Gives easier dataset managment and creates mini batches\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
        "from PIL import Image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwX-BsNbxc9B"
      },
      "cell_type": "markdown",
      "source": [
        "## Set device"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "H_e-fRSkxc9B"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu or cpu"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PuPdD_qgxc9B"
      },
      "cell_type": "markdown",
      "source": [
        "## train test split"
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "q5cD4khDxc9C"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset = ImageFolder(\"/content/data/train\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(dataset.imgs, dataset.targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# ImageLoader Class\n",
        "\n",
        "class ImageLoader(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = self.checkChannel(dataset) # some images are CMYK, Grayscale, check only RGB \n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        image = Image.open(self.dataset[item][0])\n",
        "        classCategory = self.dataset[item][1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, classCategory\n",
        "        \n",
        "    \n",
        "    def checkChannel(self, dataset):\n",
        "        datasetRGB = []\n",
        "        for index in range(len(dataset)):\n",
        "            if (Image.open(dataset[index][0]).getbands() == (\"R\", \"G\", \"B\")): # Check Channels\n",
        "                datasetRGB.append(dataset[index])\n",
        "        return datasetRGB"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IXMRiN9yxc9C"
      },
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "]) # train transform\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "]) # test transform\n",
        "\n",
        "train_dataset = ImageLoader(train_data, train_transform)\n",
        "test_dataset = ImageLoader(test_data, test_transform)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wW4jMnuTxc9D"
      },
      "cell_type": "markdown",
      "source": [
        "# Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "\n",
        "## The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FLVu978Ixc9D"
      },
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fmoKmJxGxc9E"
      },
      "cell_type": "markdown",
      "source": [
        "## Resnet50 Transfer learning technique\n",
        "\n",
        "![Structure-of-the-ResNet-50-used-for-reservoir-recognition.jpg](attachment:Structure-of-the-ResNet-50-used-for-reservoir-recognition.jpg)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ed4438c26d848618a97ed4c3b6d5c75",
            "fc0fbc1e57074419a4dffc9c6eb14989",
            "fa88b33ac680459b989576993238c57c",
            "98863a5f34a64abeaf92f6d6de7477c6",
            "1b0bf8c7790d4051a8804fe4f1c8d29d",
            "a49ff725a9de47138fc87c1c65cee31a",
            "5bf71450a65843b8867ebfeab0ca531d",
            "53e0bb9e54b14ba0b31bd27dd5b1bad1",
            "a0f6f669ddf744419136ed1bc3123ef6",
            "46088f14b1a94fea8f73c5a45de35d41",
            "7a0fefa994e24adabd7a342157b89e34"
          ]
        },
        "id": "pVDjkcxWxc9E",
        "outputId": "5bfb0400-5bc7-4981-bb3f-a5c94928b4df"
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torchvision import models\n",
        "# load pretrain model and modify...\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# If you want to do finetuning then set requires_grad = False\n",
        "# Remove these two lines if you want to train entire model,\n",
        "# and only want to load the pretrain weights.\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ed4438c26d848618a97ed4c3b6d5c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xTNrrBiwxc9F"
      },
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# Train and test\n",
        "\n",
        "def train(num_epoch, model):\n",
        "    for epoch in range(0, num_epoch):\n",
        "#         current_loss = 0.0\n",
        "#         current_corrects = 0\n",
        "        losses = []\n",
        "        model.train()\n",
        "        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n",
        "        for batch_idx, (data, targets) in loop:\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "            scores = model(data)\n",
        "            \n",
        "            loss = criterion(scores, targets)\n",
        "            optimizer.zero_grad()\n",
        "            losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, preds = torch.max(scores, 1)\n",
        "#             current_loss += loss.item() * data.size(0)\n",
        "#             current_corrects += (preds == targets).sum().item()\n",
        "#             accuracy = int(current_corrects / len(train_loader.dataset) * 100)\n",
        "            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n",
        "            loop.set_postfix(loss=loss.data.item())\n",
        "        \n",
        "        # save model\n",
        "        torch.save({ \n",
        "                    'model_state_dict': model.state_dict(), \n",
        "                    'optimizer_state_dict': optimizer.state_dict(), \n",
        "                    }, 'checpoint_epoch_'+str(epoch)+'.pt')\n",
        "\n",
        "\n",
        "        \n",
        "# model.eval() is a kind of switch for some specific layers/parts of the model that behave differently,\n",
        "# during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. \n",
        "# You need to turn off them during model evaluation, and .eval() will do it for you. In addition, \n",
        "# the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() \n",
        "# to turn off gradients computation:\n",
        "        \n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(x)\n",
        "            _, predictions = torch.max(output, 1)\n",
        "            correct += (predictions == y).sum().item()\n",
        "            test_loss = criterion(output, y)\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n",
        "    len(test_loader.dataset), \"  \", int(correct / len(test_loader.dataset) * 100), \"%\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A37ejoOpxc9F"
      },
      "cell_type": "markdown",
      "source": [
        "> CrossEntropy or other loss functions is divided by the number of elements i.e. the reduction parameter is mean by default.\n",
        "\n",
        "*     torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
        "\n",
        "> Hence, loss.item() contains the loss of entire mini-batch, but divided by the batch size. That's why loss.item() is multiplied with batch size, given by inputs.size(0), while calculating running_loss\n",
        "> (stackoverflow)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV5LlVqcxc9G",
        "outputId": "e50ac20d-fec9-4e89-8589-fa55fd09d734"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train(5, model) # train\n",
        "    test() # test"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5 process: 75: 100%|██████████| 4/4 [01:49<00:00, 27.43s/it, loss=0.371]\n",
            "Epoch 2/5 process: 75: 100%|██████████| 4/4 [01:44<00:00, 26.17s/it, loss=0.137]\n",
            "Epoch 3/5 process: 75: 100%|██████████| 4/4 [01:44<00:00, 26.16s/it, loss=0.0796]\n",
            "Epoch 4/5 process: 75: 100%|██████████| 4/4 [01:45<00:00, 26.42s/it, loss=0.636]\n",
            "Epoch 5/5 process: 75: 100%|██████████| 4/4 [01:47<00:00, 26.86s/it, loss=0.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss:  tensor(0.0018)   Accuracy:  50  /  51    98 %\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvxZLRU9xc9G",
        "outputId": "7541814a-9652-4e77-d19b-e91f93f91edd"
      },
      "cell_type": "code",
      "source": [
        "print(\"----> Loading checkpoint\")\n",
        "checkpoint = torch.load(\"./checpoint_epoch_4.pt\") # Try to load last checkpoint\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----> Loading checkpoint\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJdFL3Jhxc9G",
        "outputId": "29c657f9-e070-45c4-a7e8-7a23317660d4"
      },
      "cell_type": "code",
      "source": [
        "# Check the test set\n",
        "dataset = ImageFolder(\"/content/data/val\", \n",
        "                     transform=transforms.Compose([\n",
        "                         transforms.Resize((224, 224)), \n",
        "                         transforms.ToTensor(), \n",
        "                         transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "                     ]))\n",
        "print(dataset)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle = False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 39\n",
            "    Root location: /content/data/val\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6AHAponxc9H",
        "outputId": "e90e2e4e-1fb8-4bbf-955d-49436cb64b4e"
      },
      "cell_type": "code",
      "source": [
        "# for j, (data, labels) in enumerate(dataloader):\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for data, target in dataloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        print(f\"predicted ----> {predicted[0]}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 1\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 1\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 0\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n",
            "predicted ----> 1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PCEUpFq4xc9I"
      },
      "cell_type": "markdown",
      "source": [
        "# create a function to predict random cats and dog images"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fou1pPTqxc9I"
      },
      "cell_type": "code",
      "source": [
        "def RandomImagePrediction(filepath):\n",
        "    img_array = Image.open(filepath).convert(\"RGB\")\n",
        "    data_transforms=transforms.Compose([\n",
        "        transforms.Resize((224, 224)), \n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ])\n",
        "    img = data_transforms(img_array).unsqueeze(dim=0) # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "    load = DataLoader(img)\n",
        "    \n",
        "    for x in load:\n",
        "        x=x.to(device)\n",
        "        pred = model(x)\n",
        "        _, preds = torch.max(pred, 1)\n",
        "        print(f\"class : {preds}\")\n",
        "        if preds[0] == 1: print(f\"predicted ----> Dog\")\n",
        "        else: print(f\"predicted ----> Cat\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErtyRSDWxc9I",
        "outputId": "c185e845-d10d-4784-c151-153283d11320"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    RandomImagePrediction(\"/content/cover.jpg\") # dog image\n",
        "    RandomImagePrediction(\"/content/1bdkaxbrpo86pxd3.jpg\") # cat image\n",
        "    RandomImagePrediction(\"/content/1340392-Puppy-Baby-AnimalWolfdog-HD-Wallpaper.jpg\") # dog image"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class : tensor([1])\n",
            "predicted ----> Dog\n",
            "class : tensor([0])\n",
            "predicted ----> Cat\n",
            "class : tensor([1])\n",
            "predicted ----> Dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5afUcAn2w15"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ed4438c26d848618a97ed4c3b6d5c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc0fbc1e57074419a4dffc9c6eb14989",
              "IPY_MODEL_fa88b33ac680459b989576993238c57c",
              "IPY_MODEL_98863a5f34a64abeaf92f6d6de7477c6"
            ],
            "layout": "IPY_MODEL_1b0bf8c7790d4051a8804fe4f1c8d29d"
          }
        },
        "fc0fbc1e57074419a4dffc9c6eb14989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a49ff725a9de47138fc87c1c65cee31a",
            "placeholder": "​",
            "style": "IPY_MODEL_5bf71450a65843b8867ebfeab0ca531d",
            "value": "100%"
          }
        },
        "fa88b33ac680459b989576993238c57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53e0bb9e54b14ba0b31bd27dd5b1bad1",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0f6f669ddf744419136ed1bc3123ef6",
            "value": 102530333
          }
        },
        "98863a5f34a64abeaf92f6d6de7477c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46088f14b1a94fea8f73c5a45de35d41",
            "placeholder": "​",
            "style": "IPY_MODEL_7a0fefa994e24adabd7a342157b89e34",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 140MB/s]"
          }
        },
        "1b0bf8c7790d4051a8804fe4f1c8d29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a49ff725a9de47138fc87c1c65cee31a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf71450a65843b8867ebfeab0ca531d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53e0bb9e54b14ba0b31bd27dd5b1bad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f6f669ddf744419136ed1bc3123ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46088f14b1a94fea8f73c5a45de35d41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0fefa994e24adabd7a342157b89e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}