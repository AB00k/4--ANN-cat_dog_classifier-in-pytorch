{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUmvlUanwN5X"
   },
   "source": [
    "Import important libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-y9HE8gRsTcu"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwUbEz9SwZyK"
   },
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG_8tAmgv9gm"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUF7JrC6wgH3"
   },
   "source": [
    "pre-processing , training and importing data and evaluation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBDwW25jsONY"
   },
   "outputs": [],
   "source": [
    "# Set the device to run on GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the data using the torchvision.datasets module\n",
    "data_dir = \"data\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform)\n",
    "\n",
    "# Create data loaders to load the data in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model architecture\n",
    "model = SimpleCNN()\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze the parameters in the model to prevent backpropagation through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully-connected layer with a new layer with two outputs\n",
    "fc_input_size = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_input_size, 2)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            accuracy += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Calculate average losses and accuracy\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        accuracy = accuracy.double() / len(val_loader.dataset)\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "        print(\"Training Loss: {:.4f}\".format(train_loss))\n",
    "        print(\"Validation Loss: {:.4f}\".format(val_loss))\n",
    "        print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
